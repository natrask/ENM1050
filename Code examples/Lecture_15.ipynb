{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **In-class 15. Pickle, Automatic differentiation, PyTorch, and gradient descent.** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTRIBUTORS #\n",
    "\n",
    "This in-class exercise is to be done in pairs. Add the names of the two students in this text block.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for today ##\n",
    "\n",
    "Today we'll learn to use pytorch as a tool to perform automatic differentiation (AD). AD is special, because it allows us to take the derivatives of very complex functions and use those derivatives to perform optimization. On it's own, AD is simply a mathematical tool. When combined with ideas from artificial neural networks, it allows us to build and train models inspired by the functionality of the human brain. Today we will first get started on the basic syntax of PyTorch, and use it to fit a very simple model to data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pickle to save and load data. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to need some tools to build datasets and save them to disk. There are more sophisticated tools (typically one would work with [Pandas dataframes](https://pandas.pydata.org/) in a more serious data science setting), but we will use [Pickle](https://docs.python.org/3/library/pickle.html) to get started because it is incredibly simple to use and is fast for relatively small datasets. For very large datasets Pickle won't cut it - in that case you would use something like [HDF5](https://docs.h5py.org/en/stable/). As always, when learning a new library you will find it useful to keep a LLM open in your browser to help answer specific questions about syntax.\n",
    "\n",
    "Pickle will take *any python object* and save it to disk as a `.pkl` file (pronounced \"pickle\"). There are three key functions to understand:\n",
    "* `open`: Open up a pickle file. The first argument is the file name, and the second argument sets whether to open the file in read or write mode.\n",
    "* `pickle.dump`: Pushes the object into the pkl file. The first argument is the python object, while the second is the opened pkl file.\n",
    "* `pickle.load`: Loads the pkl file from disk. The argument is the opened pkl file.\n",
    "\n",
    "In what follows you'll be seeing us use the command `with` for the first time. `with` is a convenient way to set the scope so that it vanishes. at the end of the code block, it vanishes. \n",
    "\n",
    "The syntax is\n",
    "``` \n",
    "with OBJECT as OBJECT_NAME:\n",
    "  # do things \n",
    "```\n",
    "\n",
    "This makes it easy to make sure you don't use an object by accident outside of a certain place. In the example code below, we use `with` to make sure our pickle file is in either write mode (`'wb'`) or read mode (`'rb'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array 0: [1 2 3]\n",
      "Array 1: [4 5 6]\n",
      "Array 2: [7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Create a list of NumPy arrays\n",
    "array_list = [np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])]\n",
    "\n",
    "# Save the list of NumPy arrays to a file\n",
    "with open('array_list.pkl', 'wb') as filename:\n",
    "    pickle.dump(array_list, filename)\n",
    "\n",
    "# Load the list of NumPy arrays from the file\n",
    "with open('array_list.pkl', 'rb') as filename:\n",
    "    loaded_array_list = pickle.load(filename)\n",
    "\n",
    "# Verify the loaded data\n",
    "for i, array in enumerate(loaded_array_list):\n",
    "    print(f\"Array {i}:\", array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn.** Use the bouncy rock paper scissors code to generate 10 trajectories. Use pickle to save them as separate pkl files on your disk (e.g. `dataset_001.pkl`, `dataset_002.pkl`,...). Write code here to load all of the pkl files at once and store them as a list of numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll learn to use pytorch as a tool to perform automatic differentiation (AD). AD is special, because it allows us to take the derivatives of very complex functions and use those derivatives to perform optimization. On it's own, AD is simply a mathematical tool. When combined with ideas from artificial neural networks, it allows us to build and train models inspired by the functionality of the human brain. Today we will first get started using AD to minimize some simple functions.\n",
    "\n",
    "**Tensor syntax.** PyTorch operates on tensors, which you can think of as no different from numpy array, but that they have the \"plumbing\" in place to do automatic differentiation. The following gives some examples of how you can build tensors and interact with them - it is nearly identical to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create PyTorch tensors\n",
    "tensor_a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "tensor_b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# Transpose\n",
    "tensor_transpose = tensor_a.t()\n",
    "\n",
    "# Reshape\n",
    "tensor_reshape = tensor_a.view(4, 1)\n",
    "\n",
    "# Slicing\n",
    "tensor_slice = tensor_a[:, 1]\n",
    "\n",
    "# Concatenation - dim states which dimension to concatenate in (i.e. stack in rows vs stack in columns)\n",
    "tensor_concat = torch.cat((tensor_a, tensor_b), dim=0)\n",
    "\n",
    "# Sum\n",
    "tensor_sum = torch.sum(tensor_a)\n",
    "\n",
    "# Mean\n",
    "tensor_mean = torch.mean(tensor_a)\n",
    "\n",
    "# Element-wise sine\n",
    "tensor_sin = torch.sin(tensor_a)\n",
    "\n",
    "# Norm\n",
    "tensor_norm = torch.norm(tensor_a)\n",
    "\n",
    "# In-place addition\n",
    "tensor_a.add_(tensor_b)\n",
    "\n",
    "print(\"Transpose:\\n\", tensor_transpose)\n",
    "print(\"Reshape:\\n\", tensor_reshape)\n",
    "print(\"Slicing:\\n\", tensor_slice)\n",
    "print(\"Concatenation:\\n\", tensor_concat)\n",
    "print(\"Sum:\\n\", tensor_sum)\n",
    "print(\"Mean:\\n\", tensor_mean)\n",
    "print(\"Element-wise Sine:\\n\", tensor_sin)\n",
    "print(\"Norm:\\n\", tensor_norm)\n",
    "print(\"In-place Addition:\\n\", tensor_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taking derivatives.** The following code implements a very simple polynomial function and takes its derivative. For this there are a few parts to pay careful attention to:\n",
    "* `Requires_grad=True` when defining `x` is important, because it tells PyTorch to track future variables which depend on `x` - this is the cue to start building up the computational graph for building derivatives.\n",
    "* `y.backward` builds up the derivatives with respect to all variables that `y` depends upon which were initialized with the `requires_grad` flag.\n",
    "* `x.grad` returns the computed derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with requires_grad=True to track computations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "y = x ** 2 + 3 * x + 1\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient\n",
    "print(\"Gradient of y with respect to x:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zeroing out the gradients.**\n",
    "To make things fast, PyTorch accumulates gradients on the fly. This is because when performing the chain rule, many derivatives are aggregated across the model over and over. This means however that you need to \"clear the buffer\" before calculating a derivative a second time. The following code illustrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple function\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Initialize the tensor with requires_grad=True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Perform the first backward pass\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(\"Gradient after first backward pass:\", x.grad)\n",
    "\n",
    "# Perform the second backward pass without zeroing gradients\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(\"Gradient after second backward pass without zeroing:\", x.grad)\n",
    "\n",
    "# Zero the gradients\n",
    "x.grad.zero_()\n",
    "\n",
    "# Perform the third backward pass after zeroing gradients\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(\"Gradient after third backward pass with zeroing:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detaching tensors from the graph.** Pytorch will track every variable as you build up more and more complicated expressions. Sometimes though, you just want to do some operations on a function that don't need derivatives (e.g. you want to make some plots). For this you can use the `detach()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "y = x ** 2 + 3 * x + 1\n",
    "\n",
    "# Detach the tensor\n",
    "y_detached = y.detach()\n",
    "\n",
    "# Print the original and detached tensors\n",
    "print(\"Original tensor:\", y)\n",
    "print(\"Detached tensor:\", y_detached)\n",
    "\n",
    "# Perform an operation on the detached tensor\n",
    "y_detached += 1\n",
    "\n",
    "# Print the modified detached tensor and the original tensor\n",
    "print(\"Modified detached tensor:\", y_detached)\n",
    "print(\"Original tensor after modifying detached tensor:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going back and forth between numpy.** It is possible to interpret pytorch tensors at numpy arrays and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Convert PyTorch tensor to NumPy array\n",
    "numpy_array = tensor.numpy()\n",
    "print(\"NumPy array:\", numpy_array)\n",
    "\n",
    "# Convert NumPy array back to PyTorch tensor\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(\"PyTorch tensor from NumPy array:\", tensor_from_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example - Generating plots of PyTorch variables.** The following code will generate a plot of a polynomial function and gives examples of how to use `numpy()` and `detach()` when making plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return x ** 2 + 3 * x + 1\n",
    "\n",
    "# Generate x values between 0 and 1\n",
    "x_values = torch.linspace(0, 1, steps=100, requires_grad=True)\n",
    "\n",
    "# Compute y values\n",
    "y_values = f(x_values)\n",
    "\n",
    "# Plot the function\n",
    "#   Note: we use detach() here because we don't need derivatives of plots with respect to inputs\n",
    "plt.plot(x_values.detach().numpy(), y_values.detach().numpy(), label='y = x^2 + 3x + 1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Plot of y = x^2 + 3x + 1 for x between 0 and 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn.** Modify the previous code examples to plot the derivative of the function $f$. Confirm that you have the correct derivative by plotting the true derivative (calculated by hand)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn.** Modify that same code to take the derivative of $y = \\sin 2 \\pi x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining AD with optimizers to fit models. ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early in the semester we implemented Newton's method to find solutions to nonlinear equations.\n",
    "$$F(x) = 0 $$\n",
    "\n",
    "Recall the following formula told us how to find our next guess for a solution.\n",
    "$$x_{n+1} = x_n - \\frac{F (x_n)}{F' (x_n)}$$\n",
    "Remember also that if we instead want to minimize a function\n",
    "$$\\underset{x}{\\min}\\, G(x)$$\n",
    "that amounts to setting the first derivative to zero\n",
    "\n",
    "$$G'(x) = 0$$\n",
    "\n",
    "in which case Newton reduces to\n",
    "$$x_{n+1} = x_n - \\frac{G'(x_n)}{G''(x_n)}$$\n",
    "This requires taking two derivatives. For the toy functions we've looked at this isn't a big deal. But let's do a little exercise to see how quickly these derivatives get complicated. If you cook up the following (physically meaningless but complicated) expression:\n",
    "\n",
    "$$G(x) = \\frac{\\sin \\left[\\exp \\left( \\cos x^2 \\right)^3\\right] }{\\cos \\left[\\exp -\\left( \\tan x^4 \\right)^2 \\right]}$$\n",
    "The derivative has many terms, even after simplifying:\n",
    "$$G'(x) = \\frac{\n",
    "    \\cos \\left( \\exp \\left( \\cos x^2 \\right)^3 \\right) \\cdot \\left( \\exp \\left( \\cos x^2 \\right)^3 \\right)' \\cos \\left( \\exp \\left( -\\left( \\tan x^4 \\right)^2 \\right) \\right) - \\sin \\left( \\exp \\left( \\cos x^2 \\right)^3 \\right) \\cdot -\\sin \\left( \\exp \\left( -\\left( \\tan x^4 \\right)^2 \\right) \\right) \\cdot \\left( \\exp \\left( -\\left( \\tan x^4 \\right)^2 \\right) \\right)'\n",
    "}{\n",
    "    \\left( \\cos \\left( \\exp \\left( -\\left( \\tan x^4 \\right)^2 \\right) \\right) \\right)^2\n",
    "}$$\n",
    "\n",
    "Taking a second derivative would get even gnarlier! So as we move toward optimizing complicated nonlinear functions (like what shows up in neural networks) we want to avoid taking a second derivative if we can help it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent.** For gradient descent when minimizing a function $G$, we simplify the Newton update as follows:\n",
    "$$x_{n+1} = x_n - \\eta G'(x)$$\n",
    "\n",
    "where $\\eta << 1$ is called a **learning rate**. The idea is that the derivative $G'$ tells you which direction to go, and the learning rate can be chosen as a small number that we pick that tells us how big of a step to take. This is inefficient - it will take many more steps to get to the right answer. You will also have to play with $\\eta$ - you'll run the optimizer for some guess about what $\\eta$ should be (maybe $\\eta = 0.001$ is a good starting point), and then you'll run it again with a smaller $\\eta$ and make sure the answer doesn't change. But what you get is a method that only needs information that pops out of automatic differentiation.\n",
    "\n",
    "$\\eta$ is an example of a *hyperparameter*. A hyperparameter is a number in your code that needs to be nailed down. To contrast, we could call variables that correspond to the model itself as a parameter (for example, the polynomial coefficients when we do regression). Hyperparameters are something that changes the behavior of the model fitting (or *training* in machine learning speak) process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement gradient descent.** The following code implements gradient descent. As usual, use a LLM to get an explanation of any components that don't make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the function to be minimized\n",
    "def f(x):\n",
    "    return x ** 2 + 3 * x + 1\n",
    "\n",
    "# Initialize the tensor with requires_grad=True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "\n",
    "# Gradient descent loop\n",
    "for iteration in range(num_iterations):\n",
    "    # Compute the function value\n",
    "    y = f(x)\n",
    "    \n",
    "    # Perform backpropagation to compute gradients\n",
    "    y.backward()\n",
    "    \n",
    "    # Update the tensor using the computed gradients\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    \n",
    "    # Zero the gradients\n",
    "    x.grad.zero_()\n",
    "\n",
    "# Print the optimized value\n",
    "print(\"Optimized value of x:\", x.item())\n",
    "print(\"Minimum value of the function:\", f(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of what I got when I asked Gemini to explain the code:\n",
    "\n",
    "### Explanation\n",
    "- **Function Definition**: `f(x)` is the function to be minimized.\n",
    "- **Tensor Initialization**: `x` is initialized with `requires_grad=True`.\n",
    "- **Hyperparameters**: `learning_rate` and `num_iterations` are set.\n",
    "- **Gradient Descent Loop**:\n",
    "  - Compute the function value `y`.\n",
    "  - Perform backpropagation with `y.backward()` to compute gradients.\n",
    "  - Update `x` using the gradient and learning rate.\n",
    "  - Zero the gradients with `x.grad.zero_()`.\n",
    "- **Results**: Print the optimized value of `x` and the minimum value of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn.** In `Code examples/Lecture06.ipynb` earlier in the semester we generated a Newton solve to optimize a function. Run that for the same function `def f(x): return x ** 2 + 3 * x + 1`. Generate a log-log plot showing how the error decreases during training. On the x-axis plot the step of training, and the y-axis show the absolute value of the error. Generate several plots for different choices of learning rate, and see if you can get gradient descent to be competitive with Newton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put stuff here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jumpstart on the homework ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The homework will be pretty involved. My aim is to give you lots of time to prepare for it. It will be due a week from Wednesday, and I have cleared class on the day that it's due to allow everyone to work on it together if there are last minute questions. (**Do not leave it until then to get started - there is no way that you'll be able to complete it.**). Like the last homework, this is a larger, multi-component project that you will want to read through today and **get help at OH early**.\n",
    "\n",
    "Today if you still have time at the end of class, you can read through the problem statement and ask questions. You are in a good spot right now to make sure you collect all of your data from the simulator and have it ready to work with in Wednesday's class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning in assignments on Canvas #\n",
    "In order to submit your assignment as a pair, you need to create a group on Canvas. This will enable you to both receive the same grade for one submission.\n",
    "\n",
    "On Canvas, navigate to People > Groups > In-Class 15.\n",
    "Find an empty group and add the names of both members of the pair.\n",
    "\n",
    "Submit your work as both an ipynb and a pdf to Canvas.\n",
    "\n",
    "Save the ipynb and upload from your hard drive. Also print a pdf file to ensure the graders can see you have completed the exercise, even if there are issues with the formatting in your jupyter notebook.\n",
    "\n",
    "The student who did not submit should make sure that the group was created successfully by checking that they can also access the files on their Canvas page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
