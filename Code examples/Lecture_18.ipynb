{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRVkC7UF8XxVIaF3M6peCn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natrask/ENM1050/blob/main/Code%20examples/Lecture_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 17 - Pandas and a cookbook for a data-gathering/classification pipeline** #"
      ],
      "metadata": {
        "id": "ATHT087clnJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONTRIBUTORS #\n",
        "\n",
        "This in-class exercise is to be done in pairs. Add the names of the two students in this text block.\n"
      ],
      "metadata": {
        "id": "JuC1pXsyl3Re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of today #"
      ],
      "metadata": {
        "id": "AzxV34XYl8rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today we are going to be processing *geospatial data* - data that describes the position of a sensor on the globe. To do that, we will need a library that doesn't come installed by default in colab, called *geopandas*. Pandas, and geopandas, are both libraries for easily handling data, and act as a simple layer between the data and pytorch that will make our life easier.\n",
        "\n",
        "First, we will tell colab to install geopandas and contextily. This will take a minute."
      ],
      "metadata": {
        "id": "Jzz8Nq9DmAZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz9EvoJ4iuTt"
      },
      "outputs": [],
      "source": [
        "# First, here is a list of libraries that we'll need.\n",
        "\n",
        "!pip install geopandas contextily\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will see an example of what kind of processing geopandas allows us to do.\n",
        "\n",
        "A pandas dataframe (pd.DataFrame) holds different types of data. In the short example below, it holds data corresponding to address, latitude, and longitude for two different addresses.\n",
        "\n",
        "By wrapping it in a GeoDataFrame, we are specifying which parts of the data correspond to a geometric data point, and specify a coordinate system (in this case something called EPSG:4326). We use this to render the two points on a map of Philadelphia."
      ],
      "metadata": {
        "id": "IWhsZHZvmhJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual data\n",
        "# Assuming you have a CSV file with 'address' and 'latitude', 'longitude' columns\n",
        "data = pd.DataFrame({'address': ['123 Market St, Philadelphia, PA', '456 Walnut St, Philadelphia, PA'],\n",
        "                     'latitude': [39.9526, 39.9500],\n",
        "                     'longitude': [-75.1652, -75.1452]})\n",
        "\n",
        "# Create a GeoDataFrame from your data\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n",
        "gdf.crs = 'EPSG:4326'  # Set the coordinate system to WGS 84\n",
        "\n",
        "# Plot the map\n",
        "ax = gdf.plot(figsize=(10, 10), markersize=50, color='red')\n",
        "\n",
        "# Add a basemap\n",
        "ctx.add_basemap(ax, crs=gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "\n",
        "# Customize plot (optional)\n",
        "plt.title('Street Addresses in Philadelphia')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "__SMAFLPmlSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, geospatial data can come from many sources. As we transition to working with robots and sensors in the remainder of the course, we will see that it is easy to extract latitude and longitude with a small sensor. This can be used to assist in robotic navigation, or track the location of a passive sensor (like a weather balloon) as it drifts around in the atmosphere. If other information is gathered at the same site (for example, temperature/humidity for a weather sensor), then we can attempt to use machine learning to associate measurements with either classification or prediction.\n",
        "\n",
        "**Today's goal:** we will build up a model of the Penn campus that aims to identify what parts of campus are primarily residential vs academic."
      ],
      "metadata": {
        "id": "qtkE0zQqnTon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing Pandas #"
      ],
      "metadata": {
        "id": "MLNoAauDos8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we do that, I'm going to introduce Pandas. Pandas makes it easy to explore very large datasets. For many datasets that you would receive in practice, they would likely come prepackaged as a pandas dataframe. In this first example, we will generate a dataframe describing four people and some demographics about them."
      ],
      "metadata": {
        "id": "lvUqTTT7ovEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a DataFrame from a dictionary\n",
        "data = {'Name': ['Alice', 'Bob', 'Charlie', 'Dennis'],\n",
        "        'Age': [25, 30, 22, 18],\n",
        "        'City': ['New York', 'London', 'Paris', 'Philadelphia'],\n",
        "        'Income': [96000,55327,101101,42000]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Displaying the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "caLC-N12kX6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following sets of functions show how pandas let's use easily manipulate the data, gather subsets of the data, and summarize trends in the dataset."
      ],
      "metadata": {
        "id": "VPUxex1rpsz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Accessing columns\n",
        "print('The follow is the names in the dataset')\n",
        "print(df['Name'])  # Accessing the 'Name' column\n",
        "print('\\n')\n",
        "\n",
        "# Accessing rows by index\n",
        "print('This is the first entry in the dataset')\n",
        "print(df.loc[0])  # Accessing the first row\n",
        "print('\\n')\n",
        "\n",
        "# # Filtering data\n",
        "print('The following lists entries only over the age of 25:')\n",
        "print(df[df['Age'] > 25])  # Filtering rows where 'Age' is greater than 25\n",
        "print('\\n')\n",
        "\n",
        "# Adding a new column\n",
        "print('We can add a new column - in this case the country for each')\n",
        "df['Country'] = ['USA', 'UK', 'France','USA']\n",
        "print(df)\n",
        "print('\\n')\n",
        "\n",
        "# It is easy to grab simple properties\n",
        "print('The average age by country is:')\n",
        "print(df.groupby('Country')['Age'].mean())  # Grouping by 'Country' and calculating the mean of 'Age'\n",
        "\n",
        "# # Some other useful methods:\n",
        "# print(df.describe())  # Summary statistics of numerical columns\n",
        "# print(df.head(2))  # Displaying the first 2 rows\n",
        "# print(df.tail(2))  # Displaying the last 2 rows\n",
        "# print(df.sort_values(by='Age'))  # Sorting by 'Age'\n",
        "\n"
      ],
      "metadata": {
        "id": "mXohsG49ppYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember when using a new library to use Gemini! It won't be able to do your homework for you, but you can ask it to explain any one of these functions.\n",
        "\n",
        "**Your turn:** Ask gemini to explain df.groupby('Country')['Age']. Paste your explanation in the block below."
      ],
      "metadata": {
        "id": "H7iAQ28sruRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Put stuff here.*"
      ],
      "metadata": {
        "id": "z7lm5eV3sBEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Today's exercise - a complete data collection to ML pipeline #"
      ],
      "metadata": {
        "id": "m7EKmNU7sGxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last lecture we discussed *decision boundaries*. These are geometric boundaries that a classification model uses to assign each datapoint to a class. For example, if you wanted to classify a child as a newborn or an toddler, one could look at the childs weight and height as a clear indicator. We saw this last lecture as we used flower stems and petals as a way to identify flower species.\n",
        "\n",
        "Today we will develop a system that will split the Penn campus up into residential and non-residential areas. To do this, we will generate a list of latitude/longitude locations of buildings and label them as either a *dorm* or an *academic building* (meaning a lecture hall, laboratory, etc).\n",
        "\n",
        "We will do this by constructing a google form. The reason for this is that in your next HW assignment you will build a survey to collect data to perform a classification analysis, and you will be able to use this exercise with minor changes."
      ],
      "metadata": {
        "id": "Fx4G5cPxsM3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn.**\n",
        "1. Open maps.google.com and search for the UPenn campus.\n",
        "2. Choose a random dorm on campus (doesn't need to be yours). Right click it on the google map to get a longitude and latitude.\n",
        "3. Open up the following survey: https://forms.gle/TZ5gedEEHYo9wGmB6\n",
        "4. Enter the longitude and latitude, and mark as a *dorm*.\n",
        "5. Repeat the process but for a random academic building, and mark as a *not dorm*."
      ],
      "metadata": {
        "id": "xkC2-YGmubmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process dataset into a pandas dataframe #"
      ],
      "metadata": {
        "id": "52g6Kc7hvJq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In what's below, we will load in the output of the survey from a corresponding googlesheet, and put it into a pandas dataframe."
      ],
      "metadata": {
        "id": "FOrKlJg8vOu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "2joGrcRElWZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spreadsheet_url = 'https://docs.google.com/spreadsheets/d/1PNweaNkwxdmCo84-1duahGVg1uelcsSS0ZaL45EVIiY/edit?gid=1824420767#gid=1824420767'\n",
        "sh = gc.open_by_url(spreadsheet_url)  # or gc.open_by_key(spreadsheet_key)\n",
        "worksheet = sh.get_worksheet(0)"
      ],
      "metadata": {
        "id": "IgUmUhPqqfVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From early in the semester, we had shown how we could loop over and process that data using lists. For example:"
      ],
      "metadata": {
        "id": "_FlTwmOrvaZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all values from the worksheet as a list of lists\n",
        "data = worksheet.get_all_values()\n",
        "for entry in range(1,len(data)):\n",
        "  lat_entry = data[entry][1]\n",
        "  lon_entry = data[entry][2]\n",
        "  if data[entry][3] == 'Dorm':\n",
        "    label_entry = 1\n",
        "  else:\n",
        "    label_entry = 0\n",
        "\n",
        "  print(lat_entry,lon_entry,label_entry)"
      ],
      "metadata": {
        "id": "KIppXVrIrX_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To instead stick it in a dataframe, we first define a list of strings defining the types of data in columns, and then loop through the data pushing new entries into the database one at a time."
      ],
      "metadata": {
        "id": "026mskDBugWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create an empty DataFrame with specified column names\n",
        "columns = ['latitude', 'longitude', 'survey_label']\n",
        "df = pd.DataFrame(columns=columns)\n",
        "\n",
        "# Get all values from the worksheet as a list of lists\n",
        "data = worksheet.get_all_values()\n",
        "for entry in range(1,len(data)):\n",
        "  lat_entry = float(data[entry][1])\n",
        "  lon_entry = float(data[entry][2])\n",
        "  if data[entry][3] == 'Dorm':\n",
        "    label_entry = 1\n",
        "  else:\n",
        "    label_entry = 0\n",
        "\n",
        "  df.loc[len(df)] = [lat_entry, lon_entry, label_entry]"
      ],
      "metadata": {
        "id": "xyuhGRIorhOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following shows how easy it is now to pull out arrays corresponding to different pairs of values."
      ],
      "metadata": {
        "id": "7wJeKq1IydvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('We can pull out pairs of latitude and longitude by listing column names in square brackets')\n",
        "print(df[['latitude','longitude']])\n",
        "print('\\n')\n",
        "\n",
        "print('We can pull these out in numpy format by appending .values at the end.')\n",
        "print(df[['latitude','longitude']].values)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "cojT4jxeyXr1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn.** Modify the code in the above block to print out a numpy array corresponding to:\n",
        "1. just latitudes\n",
        "2. latitudes and label_entry\n",
        "3. latitutude, longitudes, and label_entry"
      ],
      "metadata": {
        "id": "a2OZf9r6w115"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put stuff here"
      ],
      "metadata": {
        "id": "2sEPnYIexEJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch cookbook step 1 - Load data ##"
      ],
      "metadata": {
        "id": "zavQj7OVxI9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will visualize the data. This is where we need to use gpd. Don't work about the details of this, although you can click the Gemini button in the top right of colab and ask Gemini to \"explain what a geodataframe is\" to learning more. For now though, you can take this as a visualization showing where we have collected data."
      ],
      "metadata": {
        "id": "dRBdTsyHxNXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: superimpose a scatter plot of street addresses over a map of philadelphia\n",
        "\n",
        "# !pip install geopandas contextily\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx\n",
        "\n",
        "# Replace with your actual data\n",
        "# Assuming you have a CSV file with 'address' and 'latitude', 'longitude' columns\n",
        "data = df\n",
        "\n",
        "# Create a GeoDataFrame from your data\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n",
        "gdf.crs = 'EPSG:4326'  # Set the coordinate system to WGS 84\n",
        "\n",
        "# Plot the map\n",
        "# ax = gdf.plot(figsize=(10, 10), markersize=50, color='red')\n",
        "ax = gdf.plot(figsize=(10, 10), markersize=50, column='survey_label', cmap = 'Spectral', legend=False)\n",
        "\n",
        "# Add a basemap\n",
        "ctx.add_basemap(ax, crs=gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "\n",
        "# Customize plot (optional)\n",
        "plt.title('Charting out buildings around Penn')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oFBQU0Yqr5v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to stick this data into a pytorch tensor. You'll see here what's so nice about pandas - we can easily grab different inputs/outputs from the database and drop them in.\n",
        "\n",
        "We will want to find a model that predicts *survey_label* from *latitude* and *longitude*. That means that X_in (the input to the model) should be a size [Ndata,2] tensor, and we should process the survey_labels into a size [Ndata,2] one-hot tensor.\n",
        "\n",
        "As mentioned in last class, we always want to rescale our data so that the inputs of neural networks are between 0 and 1, or gradient descent won't work well."
      ],
      "metadata": {
        "id": "I1fKnK12xjpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load latitude and longitude into pytorch tensor\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Assuming your DataFrame 'df' has 'latitude' and 'longitude' columns\n",
        "X_in = torch.tensor(df[['latitude', 'longitude']].values,dtype=torch.float32)\n",
        "y_data = torch.tensor(df['survey_label'].values)\n",
        "\n",
        "# Rescale X_in to [0,1]\n",
        "datamin = X_in.min(dim=0)[0]\n",
        "datamax = X_in.max(dim=0)[0]\n",
        "X_in = (X_in - datamin) / (datamax - datamin)\n",
        "\n",
        "#convert to one-hot encoding, considering we have 2 classes\n",
        "y_data_onehot = torch.zeros(y_data.shape[0],2)\n",
        "y_data_onehot[torch.arange(y_data.shape[0]),y_data.long()] = 1\n",
        "\n",
        "print(\"Latitude Tensor:\", X_in[:,0])\n",
        "print(\"Longitude Tensor:\", X_in[:,1])\n",
        "print('One hot of classes:', y_data_onehot)"
      ],
      "metadata": {
        "id": "uGOmBpeqwPn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Build PyTorch model ##"
      ],
      "metadata": {
        "id": "LcUC47WYyZJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can copy and paste from one of the classification example networks in the last class. For this example, we need to take in two inputs and output two class labels. I made a neural network with *two hidden layers* - this will be more powerful than the single hidden layer we've used so far."
      ],
      "metadata": {
        "id": "Ekfd0dUwyb3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class dorm_ClassificationMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(dorm_ClassificationMLP, self).__init__()\n",
        "        self.Nneurons = 10                        # **ten** internal neurons\n",
        "        self.hidden = nn.Linear(2, self.Nneurons) # **two** input neurons\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden2 = nn.Linear(self.Nneurons, self.Nneurons) # **ten** input to **ten** output\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.output = nn.Linear(self.Nneurons, 2) # **two** output neurons\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.output(self.relu2(self.hidden2(self.relu(self.hidden(x))))))"
      ],
      "metadata": {
        "id": "dxbkQuMDxyQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 - Initialize model and optimizer ##"
      ],
      "metadata": {
        "id": "ZSnuV7Kiyyus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is standard - just copied and pasted again from last class."
      ],
      "metadata": {
        "id": "nrQ0nLzey24V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the model\n",
        "model = dorm_ClassificationMLP()\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "RQdoUWIo0Ufi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Train the model ##"
      ],
      "metadata": {
        "id": "6YJ49l_xzLYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again - this is standard and copied from the last class examples."
      ],
      "metadata": {
        "id": "a4EH-3Y_zN6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_out = model(X_in)\n",
        "    loss = criterion(y_out, y_data_onehot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', epoch, 'Loss:', loss.item())"
      ],
      "metadata": {
        "id": "Te8pZwB102_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Post-process results. ##"
      ],
      "metadata": {
        "id": "YJ0Byqy_zRfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we'll check a few predictions and make sure that the output probability is similar to the training probability."
      ],
      "metadata": {
        "id": "qd3QXG7gzU6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y_out = model(X_in)\n",
        "    for i in range(X_in.shape[0]):\n",
        "        print('(predicted probability/true probability): ',y_out[i,:].detach().numpy(),y_data_onehot[i,:].detach().numpy())\n"
      ],
      "metadata": {
        "id": "OWaHgkEF1Hpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I want to visualize the decision boundary that it's using to split dorms and non-dorms apart. To do this, I'm going to copy and paste the code from last class in order to build a contour plot with the data overlaid."
      ],
      "metadata": {
        "id": "XQDP7w1XzjOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a contour plot on a\n",
        "x_min, x_max = X_in[:, 0].min() - 0.2, X_in[:, 0].max() + 0.2\n",
        "y_min, y_max = X_in[:, 1].min() - 0.2, X_in[:, 1].max() + 0.2\n",
        "\n",
        "#generate a grid of points between min and max\n",
        "xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 200), torch.linspace(y_min, y_max, 200))\n",
        "X_grid = torch.stack([xx.flatten(), yy.flatten()], 1)\n",
        "\n",
        "#calculate the model output for the grid\n",
        "with torch.no_grad():\n",
        "    y_out = model(X_grid)\n",
        "    y_out = torch.argmax(y_out, dim=1)\n",
        "\n",
        "# Create a contour plot\n",
        "plt.contour(xx.numpy(), yy.numpy(), y_out.view(200, 200).numpy(), alpha=0.5)\n",
        "\n",
        "# Add scatter plot of the data points (optional)\n",
        "plt.scatter(X_in[:, 0].numpy(), X_in[:, 1].numpy(), c=y_data.numpy(), cmap='viridis')\n",
        "\n",
        "# Add labels and title (optional)\n",
        "plt.xlabel('Latitude')\n",
        "plt.ylabel('Longitude')\n",
        "plt.title('Decision Boundary for Dorm Classification')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gVwrYs-32CDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z-ULNXAH5CjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a CSV file with 'address' and 'latitude', 'longitude' columns\n",
        "data = df\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n",
        "gdf.crs = 'EPSG:4326'  # Set the coordinate system to WGS 84\n",
        "\n",
        "#Rescale the data back into lat/long coordinates and plot\n",
        "scaledX = X_grid[:,0]*(datamax[0]-datamin[0])+datamin[0]\n",
        "scaledY = X_grid[:,1]*(datamax[1]-datamin[1])+datamin[1]\n",
        "contour_data = pd.DataFrame({'latitude': scaledX, 'longitude': scaledY, 'prediction': y_out.numpy()})\n",
        "contour_gdf = gpd.GeoDataFrame(\n",
        "    contour_data, geometry=gpd.points_from_xy(contour_data.longitude, contour_data.latitude))\n",
        "contour_gdf.crs = 'EPSG:4326'\n",
        "\n",
        "# # Plot the map with the contour plot superimposed\n",
        "ax = contour_gdf.plot(figsize=(10,10), markersize=1, column='prediction', cmap='Spectral', alpha=0.3, legend=False)\n",
        "gdf.plot(ax=ax, markersize=50, column='survey_label', cmap='Spectral', legend=False)\n",
        "\n",
        "# # Add a basemap\n",
        "ctx.add_basemap(ax, crs=gdf.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
        "\n",
        "# # Customize plot (optional)\n",
        "plt.title('Charting out buildings around Penn')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NmFxrlxn4gZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the decision boundary more explicitly in the unscaled coordinates so we can compare more easily to the map."
      ],
      "metadata": {
        "id": "whk9P-7f4bJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate contours of decision boundary\n",
        "scaledxx = (xx*(datamax[0]-datamin[0])+datamin[0]).numpy()\n",
        "scaledyy = (yy*(datamax[1]-datamin[1])+datamin[1]).numpy()\n",
        "plt.contourf(scaledxx, scaledyy, y_out.view(200, 200).numpy(), alpha=0.5)\n",
        "\n",
        "print(data['latitude'].values)\n",
        "plt.scatter(data['latitude'].values, data['longitude'].values, c=y_data.numpy(), cmap='Spectral')\n",
        "plt.xlabel('Latitude')\n",
        "plt.ylabel('Longitude')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "35idF_y--lBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}